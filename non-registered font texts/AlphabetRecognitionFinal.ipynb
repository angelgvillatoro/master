{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/sakshibutala/CNN_AlphabetRecognition  \n",
    "\n",
    "https://towardsdatascience.com/building-and-deploying-an-alphabet-recognition-system-7ab59654c676\n",
    "## Alphabet Recognition System using Convolutional Neural Network (CNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convolutional Neural Network (CNN) is a Deep Learning Algorithm widely used for character recognition. This algorithm identifies the alphabet from the given input image.\n",
    "\n",
    "The accuracy achieved using this algorithm is 93.42%.\n",
    "\n",
    "## 1. Anvil Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'import anvil.server\\nimport anvil.media\\nanvil.server.connect(\"44STZQZQTYAAVHHWGRPMXDRN-P5VUQGPD3HW5UMWJ\")\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''import anvil.server\n",
    "import anvil.media\n",
    "anvil.server.connect(\"44STZQZQTYAAVHHWGRPMXDRN-P5VUQGPD3HW5UMWJ\")\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\an199\\anaconda3\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.preprocessing import image\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Activation\n",
    "import os\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Defining the Model Architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\an199\\anaconda3\\Lib\\site-packages\\keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\an199\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\pooling\\max_pooling2d.py:161: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\an199\\anaconda3\\Lib\\site-packages\\keras\\src\\optimizers\\__init__.py:309: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 15, 15, 32)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 6, 6, 32)          0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 1152)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 128)               147584    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 26)                3354      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 161082 (629.23 KB)\n",
      "Trainable params: 161082 (629.23 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(32, (3, 3), input_shape = (32,32,3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "\n",
    "model.add(Conv2D(32, (3, 3), activation = 'relu'))\n",
    "model.add(MaxPooling2D(pool_size = (2, 2)))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units = 128, activation = 'relu'))\n",
    "model.add(Dense(units = 26, activation = 'softmax'))\n",
    "\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Importing the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 501 images belonging to 26 classes.\n",
      "Found 260 images belonging to 26 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
    "                                   shear_range = 0.2,\n",
    "                                   zoom_range = 0.2,\n",
    "                                   horizontal_flip = True)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    directory = 'Training',\n",
    "    target_size = (32,32),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    "\n",
    ")\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    directory = 'Testing',\n",
    "    target_size = (32,32),\n",
    "    batch_size = 32,\n",
    "    class_mode = 'categorical'\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\an199\\AppData\\Local\\Temp\\ipykernel_6492\\2069286868.py:1: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.\n",
      "  history = model.fit_generator(train_generator,\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "WARNING:tensorflow:From c:\\Users\\an199\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\tf_utils.py:492: The name tf.ragged.RaggedTensorValue is deprecated. Please use tf.compat.v1.ragged.RaggedTensorValue instead.\n",
      "\n",
      "WARNING:tensorflow:From c:\\Users\\an199\\anaconda3\\Lib\\site-packages\\keras\\src\\engine\\base_layer_utils.py:384: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.1307 - accuracy: 0.1317 WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 16 batches). You may need to use the repeat() function when building your dataset.\n",
      "16/16 [==============================] - 672s 42s/step - loss: 3.1307 - accuracy: 0.1317 - val_loss: 2.8998 - val_accuracy: 0.3846\n",
      "Epoch 2/3\n",
      "16/16 [==============================] - 3s 176ms/step - loss: 2.1747 - accuracy: 0.5250\n",
      "Epoch 3/3\n",
      "16/16 [==============================] - 3s 166ms/step - loss: 1.0240 - accuracy: 0.6846\n"
     ]
    }
   ],
   "source": [
    "history = model.fit_generator(train_generator,\n",
    "                         steps_per_epoch = 16,\n",
    "                         epochs = 3,\n",
    "                         validation_data = test_generator,\n",
    "                         validation_steps = 16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Saving/Loading the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pickle.dump(model, open('CNN_model.sav', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = pickle.load(open('CNN_model.sav','rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Testing the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_result(result):\n",
    "    if result[0][0] == 1:\n",
    "        return('a')\n",
    "    elif result[0][1] == 1:\n",
    "        return ('b')\n",
    "    elif result[0][2] == 1:\n",
    "        return ('c')\n",
    "    elif result[0][3] == 1:\n",
    "        return ('d')\n",
    "    elif result[0][4] == 1:\n",
    "        return ('e')\n",
    "    elif result[0][5] == 1:\n",
    "        return ('f')\n",
    "    elif result[0][6] == 1:\n",
    "        return ('g')\n",
    "    elif result[0][7] == 1:\n",
    "        return ('h')\n",
    "    elif result[0][8] == 1:\n",
    "        return ('i')\n",
    "    elif result[0][9] == 1:\n",
    "        return ('j')\n",
    "    elif result[0][10] == 1:\n",
    "        return ('k')\n",
    "    elif result[0][11] == 1:\n",
    "        return ('l')\n",
    "    elif result[0][12] == 1:\n",
    "        return ('m')\n",
    "    elif result[0][13] == 1:\n",
    "        return ('n')\n",
    "    elif result[0][14] == 1:\n",
    "        return ('o')\n",
    "    elif result[0][15] == 1:\n",
    "        return ('p')\n",
    "    elif result[0][16] == 1:\n",
    "        return ('q')\n",
    "    elif result[0][17] == 1:\n",
    "        return ('r')\n",
    "    elif result[0][18] == 1:\n",
    "        return ('s')\n",
    "    elif result[0][19] == 1:\n",
    "        return ('t')\n",
    "    elif result[0][20] == 1:\n",
    "        return ('u')\n",
    "    elif result[0][21] == 1:\n",
    "        return ('v')\n",
    "    elif result[0][22] == 1:\n",
    "        return ('w')\n",
    "    elif result[0][23] == 1:\n",
    "        return ('x')\n",
    "    elif result[0][24] == 1:\n",
    "        return ('y')\n",
    "    elif result[0][25] == 1:\n",
    "        return ('z')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "filename = r'Testing\\e\\25.png'\n",
    "test_image = image.load_img(filename, target_size = (32,32))\n",
    "plt.imshow(test_image)\n",
    "test_image = image.img_to_array(test_image)\n",
    "test_image = np.expand_dims(test_image, axis = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 584ms/step\n",
      "Predicted Alphabet is: e\n"
     ]
    }
   ],
   "source": [
    "result = model.predict(test_image)\n",
    "result = get_result(result)\n",
    "print ('Predicted Alphabet is: {}'.format(result))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Predicting the Alphabet from the Input Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This part of code is receives the input image from the anvil website and returns the predicted alphabet back to the website."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'anvil' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;129m@anvil\u001b[39m\u001b[38;5;241m.\u001b[39mserver\u001b[38;5;241m.\u001b[39mcallable\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmodel_run\u001b[39m(path):\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m anvil\u001b[38;5;241m.\u001b[39mmedia\u001b[38;5;241m.\u001b[39mTempFile(path) \u001b[38;5;28;01mas\u001b[39;00m filename:\n\u001b[0;32m      4\u001b[0m         test_image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mload_img(filename, target_size \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m32\u001b[39m,\u001b[38;5;241m32\u001b[39m))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'anvil' is not defined"
     ]
    }
   ],
   "source": [
    "'''@anvil.server.callable\n",
    "def model_run(path):\n",
    "    with anvil.media.TempFile(path) as filename:\n",
    "        test_image = image.load_img(filename, target_size = (32,32))\n",
    "        test_image = image.img_to_array(test_image)\n",
    "        test_image = np.expand_dims(test_image, axis = 0)\n",
    "        result = model.predict(test_image)\n",
    "        result = get_result(result)\n",
    "        return ('Predicted Alphabet is: {}'.format(result))\n",
    "'''     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
